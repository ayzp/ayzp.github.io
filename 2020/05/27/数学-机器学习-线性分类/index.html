<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"ayzp.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="四 线性分类4.1 简介4.1.1 思维导图 线性分类-思维导图">
<meta property="og:type" content="article">
<meta property="og:title" content="数学-机器学习-线性分类">
<meta property="og:url" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/index.html">
<meta property="og:site_name" content="AYZP">
<meta property="og:description" content="四 线性分类4.1 简介4.1.1 思维导图 线性分类-思维导图">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/%E5%9B%9B+%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB.png">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%96%B9%E6%B3%95%E5%88%86%E7%B1%BB-1589592332538.png">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200516094833745.png">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB-%E8%83%8C%E6%99%AF2.png">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200516100944785.png">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200516105906642.png">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB-%E6%84%9F%E7%9F%A5%E6%9C%BA.jpg">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200520111407167.png">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/IMG_20200520_103115-1.jpg">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200520111211834.png">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200520110035026.png">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200520110149956.png">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200520113310220.png">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200520113500701.png">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/c9fcc3cec3fdfc03f23fbf16d73f8794a5c226dc.png">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200521084144591.png">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200521084258016.png">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200522083824880.png">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200522084923196.png">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200522085259.jpg1.jpg">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200522085726438.png">
<meta property="og:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200522092643160.png">
<meta property="article:published_time" content="2020-05-27T00:57:53.000Z">
<meta property="article:modified_time" content="2020-05-27T02:38:18.405Z">
<meta property="article:author" content="Ayzp">
<meta property="article:tag" content="数学">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/%E5%9B%9B+%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB.png">

<link rel="canonical" href="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>数学-机器学习-线性分类 | AYZP</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">AYZP</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-cv">

    <a href="/CV/" rel="section"><i class="fa fa-address-card fa-fw"></i>CV</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-set">

    <a href="/set/" rel="section"><i class="fa fa-briefcase fa-fw"></i>set</a>

  </li>
        <li class="menu-item menu-item-notes">

    <a href="/AyzpNotes/" rel="section"><i class="fa fa-sticky-note fa-fw"></i>notes</a>

  </li>
        <li class="menu-item menu-item-mission">

    <a href="/Mission/" rel="section"><i class="fa fa-tasks fa-fw"></i>Mission</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/headphoto.png">
      <meta itemprop="name" content="Ayzp">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AYZP">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          数学-机器学习-线性分类
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-05-27 08:57:53 / Modified: 10:38:18" itemprop="dateCreated datePublished" datetime="2020-05-27T08:57:53+08:00">2020-05-27</time>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>9.2k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>8 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="四-线性分类"><a href="#四-线性分类" class="headerlink" title="四 线性分类"></a>四 线性分类</h1><h2 id="4-1-简介"><a href="#4-1-简介" class="headerlink" title="4.1 简介"></a>4.1 简介</h2><h3 id="4-1-1-思维导图"><a href="#4-1-1-思维导图" class="headerlink" title="4.1.1 思维导图"></a>4.1.1 思维导图</h3><p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/四+线性分类.png" alt="四+线性分类"></p>
<p><a href="https://naotu.baidu.com/file/3b9373a03c5e87e60fd6930284150dde" target="_blank" rel="noopener">线性分类-思维导图</a></p>
<a id="more"></a>
<h3 id="4-2-2-线性分类"><a href="#4-2-2-线性分类" class="headerlink" title="4.2.2 线性分类"></a>4.2.2 线性分类</h3><p>线性回归$f(w,b)=w^{T}x+b, x \in {R^p}$是机器学习的核心。线性回归有三个属性：<code>线性</code>、<code>全局性</code>、<code>数据未加工</code>。其他机器学习的方式都是打破这三个属性的某一个或某几个而提出的。这这些和线性回归共同构成了机器学习方法。</p>
<p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/机器学习-方法分类-1589592332538.png" alt="机器学习-方法分类"></p>
<p>线性分类是打破了线性回归的全局非线性，线性回归一般是通过线性回归函数得出结论后直接输出，而线性分类是将线性回归得到的函数输入激活函数，激活函数是非线性的。</p>
<p>线性分类分为硬分类和软分类。其中硬分类是输出结果就一个集合，集合元素仅有两个0和1。而软分类输出结果则是0-1的一个区间，概率。软分类可以分为生成式和判别式。其中大名鼎鼎的logistic regression就是软分类，大家不要被它的名字误导了，虽然它叫回归，实际上它是用来处理分类问题的。</p>
<p>另外，我们还可以从降维的角度来考虑，原本是多维，然后降维或投影到0,1上来了</p>
<h2 id="4-2-内容"><a href="#4-2-内容" class="headerlink" title="4.2 内容"></a>4.2 内容</h2><h3 id="4-2-1-背景"><a href="#4-2-1-背景" class="headerlink" title="4.2.1 背景"></a>4.2.1 背景</h3><blockquote>
<p>Background</p>
</blockquote>
<p>线性回归是机器学习的核心。线性回归有三个属性：<code>线性</code>、<code>全局性</code>、<code>数据未加工</code>。其他机器学习的方式都是打破这三个属性的某一个或某几个而提出的。这这些和线性回归共同构成了机器学习方法。</p>
<p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200516094833745.png" alt="image-20200516094833745"></p>
<p>线性分类打破的是非线性，将得到的线性回归函数$f(w,b)=w^{T}x+b, x \in {R^p}$作为激活函数的输入，输出分类结果。分类又可以分为硬分类和软分类。</p>
<p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/线性分类-背景2.png" alt="线性分类-背景2"></p>
<p>另外，我们还可以从降维的角度来考虑，原本是多维，然后降维或投影到0,1上来了.</p>
<p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200516100944785.png" alt="image-20200516100944785"></p>
<h3 id="4-2-2-感知机"><a href="#4-2-2-感知机" class="headerlink" title="4.2.2 感知机"></a>4.2.2 感知机</h3><blockquote>
<p>perceptron</p>
<p>属于，线性分类，硬分类方法</p>
</blockquote>
<h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p><strong>思想：</strong><code>错误驱动</code></p>
<p><strong>策略：</strong><code>Loss Function: 被错误分类点的个数</code></p>
<h4 id="A-已知"><a href="#A-已知" class="headerlink" title="A 已知"></a>A 已知</h4><p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200516105906642.png" alt="image-20200516105906642"></p>
<h4 id="B-求"><a href="#B-求" class="headerlink" title="B 求"></a>B 求</h4><p>分类成<code>+1 -1</code>两类</p>
<h4 id="C-解"><a href="#C-解" class="headerlink" title="C 解"></a>C 解</h4><p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/线性分类-感知机.jpg" alt=" "></p>
<p>一般使用感知机算法有个前提，那就是数据线性可分，如果数据线性不可分，那就可以用<code>pocket algorithm</code>，它允许错误分类存在。</p>
<h4 id="D-收获"><a href="#D-收获" class="headerlink" title="D 收获"></a>D 收获</h4><p>感知机是线性分类硬输出的一种方法，其思想为<code>错误驱动</code>，就是使得<code>被错误分类的样本点数</code><strong>最少</strong>。是一个很简单的算法，其实现算法使用随机梯度算法SGD进行，通过SGD确定$\Delta w$的大小。</p>
<p>需要实践下这个算法才有感觉，现在还没有感觉。主要问题出现在不理解<script type="math/tex">y _{i} w^{T} x _{i}</script>的具体含义，需要一个实际的例子来说明。</p>
<h3 id="4-2-3-线性判别分析LDA"><a href="#4-2-3-线性判别分析LDA" class="headerlink" title="4.2.3 线性判别分析LDA"></a>4.2.3 线性判别分析LDA</h3><blockquote>
<p>Fisher</p>
<p>属于，线性分类，硬分类方法</p>
</blockquote>
<h4 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h4><h5 id="思想："><a href="#思想：" class="headerlink" title="思想："></a><strong>思想：</strong></h5><p><code>类内小、类间大</code>，<code>高内聚、低耦合</code>，<code>降维</code></p>
<p>从降维角度出发，将样本点投影到一维空间上去。</p>
<p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200520111407167.png" alt="image-20200520111407167" style="zoom:25%;"></p>
<h5 id="投影"><a href="#投影" class="headerlink" title="投影"></a>投影</h5><p>向量$x _{i}$在向量$w$上的投影，可以写成数学式子：</p>
<script type="math/tex; mode=display">
\begin{align}
w^{T}x _{i}
\end{align}</script><p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/IMG_20200520_103115-1.jpg" alt="IMG_20200520_103115-1"></p>
<h4 id="A-线性判别分析——模型定义"><a href="#A-线性判别分析——模型定义" class="headerlink" title="A 线性判别分析——模型定义"></a>A 线性判别分析——模型定义</h4><h5 id="已知"><a href="#已知" class="headerlink" title="已知"></a>已知</h5><p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200520111211834.png" alt="image-20200520111211834" style="zoom: 25%;"></p>
<p><strong>思想：类内小、类间大</strong></p>
<h5 id="求"><a href="#求" class="headerlink" title="求"></a>求</h5><p>投影后的目标函数$J(w)$，根据思想<code>类内小、类间大</code>可写出投影后目标函数$J(w)$:</p>
<p>目标函数：</p>
<script type="math/tex; mode=display">
\begin{align}
J(w) = \frac{ { { {({ {\bar z}_1} - { {\bar z}_2})}^2}}}{ { {s_1} + {s_2}}}
\end{align}</script><script type="math/tex; mode=display">
\begin{align}
\hat w = \mathop {\arg \max }\limits_w J(w)
\end{align}</script><p>$\bar z$是样本点投影均值，均值反映类间距离，类间距离越大，均值越大</p>
<p>$\bar s$是样本点投影方差，方差反映类内距离，类内距离越小，方差越小</p>
<p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200520110035026.png" alt="image-20200520110035026" style="zoom: 25%;"></p>
<h5 id="解"><a href="#解" class="headerlink" title="解"></a>解</h5><p>得出投影前最终$J(w)$模型为：</p>
<script type="math/tex; mode=display">
\begin{align} J(w) = \frac{ { {w^T}({ {\bar x}_{c1}} - { {\bar x}_{c2}}){ {({ {\bar x}_{c1}} - { {\bar x}_{c2}})}^T}w}}{ { {w^T}({s_{c1}} + {s_{c2}})w}} \end{align}</script><p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200520110149956.png" alt="image-20200520110149956" style="zoom:25%;"></p>
<h4 id="B-线性判别分析——模型求解"><a href="#B-线性判别分析——模型求解" class="headerlink" title="B 线性判别分析——模型求解"></a>B 线性判别分析——模型求解</h4><h5 id="已知-1"><a href="#已知-1" class="headerlink" title="已知"></a>已知</h5><script type="math/tex; mode=display">
\begin{align} J(w) = \frac{ { {w^T}({ {\bar x}_{c1}} - { {\bar x}_{c2}}){ {({ {\bar x}_{c1}} - { {\bar x}_{c2}})}^T}w}}{ { {w^T}({s_{c1}} + {s_{c2}})w}}{\rm{ = }}\frac{ { {w^T}{S_b}w}}{ { {w^T}{S_w}w}} \end{align}</script><p>$S_{b}$：between-class 类间方差 $p*p维$</p>
<p>$S_{w}$：between-class 类内方差 $p*p维$</p>
<h5 id="求-1"><a href="#求-1" class="headerlink" title="求"></a>求</h5><script type="math/tex; mode=display">
\begin{align} \hat w = \mathop {\arg \max }\limits_w J(w) \end{align}</script><h5 id="解-1"><a href="#解-1" class="headerlink" title="解"></a>解</h5><script type="math/tex; mode=display">
\begin{align} J(w) = \frac{ { {w^T}{S_b}w}}{ { {w^T}{S_w}w}} = {w^T}{S_b}w \cdot {({w^T}{S_w}w)^{ {\rm{ - }}1}} \end{align}</script><p>其中，</p>
<script type="math/tex; mode=display">
\begin{align} \begin{array}{l}
{S_b} = ({ {\bar x}_{c1}} - { {\bar x}_{c2}}){({ {\bar x}_{c1}} - { {\bar x}_{c2}})^T}\\
{S_w} = {s_{c1}} + {s_{c2}}
\end{array} \end{align}</script><p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200520113310220.png" alt="image-20200520113310220" style="zoom:25%;"></p>
<p>为什么<code>只关注w的方向，不关心大小呢？</code></p>
<p>&emsp;&emsp;因为线性判别分析在应用中存在很大局限性，只需要理解下它的思想即可。</p>
<p>从维数可以看出 <script type="math/tex">w^{T}S_{w}w</script>和 <script type="math/tex">w^{T}S_{b}w</script> 都是一个 $1*1$ 的实数</p>
<p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200520113500701.png" alt="image-20200520113500701" style="zoom:25%;"></p>
<p>最后参数$w$的结果表明，<script type="math/tex">w</script>的方向与：不同类内方差和 <script type="math/tex">S_w</script> 、不同类的均值差 <script type="math/tex">\bar x_{c1} - \bar x_{c2}</script> 有关。若<script type="math/tex">S_w</script>为对角阵，各向同性，那么 <script type="math/tex">w</script> 的方向仅与 <script type="math/tex">\bar x_{c1} - \bar x_{c2}</script> 有关。</p>
<h4 id="C-收获"><a href="#C-收获" class="headerlink" title="C 收获"></a>C 收获</h4><p>线性判别分析属于线性分类硬输出的一种方式，线性判别分析的思想就是，将样本点投影到一维方向上去，使得投影之后的结果<code>类内小、类间大</code>。</p>
<h3 id="4-2-4-逻辑回归"><a href="#4-2-4-逻辑回归" class="headerlink" title="4.2.4 逻辑回归"></a>4.2.4 逻辑回归</h3><blockquote>
<p>Logistic Regression</p>
<p>属于，线性分类，软分类，判别式方法</p>
</blockquote>
<h4 id="背景-2"><a href="#背景-2" class="headerlink" title="背景"></a>背景</h4><p>软分类之判别式方法，就是对$P(Y|X)$直接进行建模求解，求出$P(Y=1|X)和P(Y=0|X)$，其中$X$为输入数据，$Y$为输出分类。</p>
<p>有时候我们只要得到一个类别的概率，那么我们需要一种能输出 $[0,1]$ 区间的值的函数。考虑两分类模型，我们利用判别模型，希望对 $p(C|x)$ 建模，利用贝叶斯定理：</p>
<script type="math/tex; mode=display">
\begin{align} p(C_1|x)=\frac{p(x|C_1)p(C_1)}{p(x|C_1)p(C_1)+p(x|C_2)p(C_2)} \end{align}</script><p>取 $a=\ln\frac{p(x|C_1)p(C_1)}{p(x|C_2)p(C_2)}$，于是：</p>
<script type="math/tex; mode=display">
\begin{align} p(C_1|x)=\frac{1}{1+\exp(-a)} \end{align}</script><p>上面的式子叫 Logistic Sigmoid 函数，其参数表示了两类联合概率比值的对数。在判别式中，不关心这个参数的具体值，模型假设直接对 $a$ 进行。</p>
<p>Logistic 回归的模型假设是：</p>
<script type="math/tex; mode=display">
\begin{align} a=w^Tx \end{align}</script><p>于是，通过寻找 $  w$ 的最佳值可以得到在这个模型假设下的最佳模型。概率判别模型常用最大似然估计的方式来确定参数。</p>
<h4 id="A-已知-1"><a href="#A-已知-1" class="headerlink" title="A 已知"></a>A 已知</h4><script type="math/tex; mode=display">
\begin{align} \begin{array}{l}
Data:\{ ({x_i},{y_i})\} _{i = 1}^N\\
{x_i} \in {R^p},{y_i} \in \{ 0,1\} 
\end{array} \end{align}</script><p>引入sigmoid函数</p>
<p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/c9fcc3cec3fdfc03f23fbf16d73f8794a5c226dc.png" alt="img" style="zoom: 67%;"></p>
<script type="math/tex; mode=display">
\begin{align} \sigma (z) = \frac{1}{ {1 + {e^{ - z}}}} \end{align}</script><h4 id="B-求-1"><a href="#B-求-1" class="headerlink" title="B 求"></a>B 求</h4><script type="math/tex; mode=display">
\begin{align} \hat w = \mathop {\arg \max }\limits_w P(Y|X) \end{align}</script><h4 id="C-解-1"><a href="#C-解-1" class="headerlink" title="C 解"></a>C 解</h4><p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200521084144591.png" alt="image-20200521084144591" style="zoom: 67%;"></p>
<p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200521084258016.png" alt="image-20200521084258016" style="zoom: 67%;"></p>
<script type="math/tex; mode=display">
\begin{align} \hat{w}=\mathop{argmax}_wJ(w)=\mathop{argmax}_w\sum\limits_{i=1}^N(y_i\log p_1+(1-y_i)\log p_0) \end{align}</script><p>注意到，这个表达式是交叉熵表达式的相反数乘 $N$，MLE 中的对数也保证了可以和指数函数相匹配，从而在大的区间汇总获取稳定的梯度。</p>
<p>对这个函数求导数，注意到：</p>
<script type="math/tex; mode=display">
\begin{align} p_1'=(\frac{1}{1+\exp(-a)})'=p_1(1-p_1) \end{align}</script><p>则：</p>
<script type="math/tex; mode=display">
\begin{align} J'(w)=\sum\limits_{i=1}^Ny_i(1-p_1)x_i-p_1x_i+y_ip_1x_i=\sum\limits_{i=1}^N(y_i-p_1)x_i \end{align}</script><p>由于概率值的非线性，放在求和符号中时，这个式子无法直接求解。于是在实际训练的时候，和感知机类似，也可以使用不同大小的批量随机梯度上升（对于最小化就是梯度下降）来获得这个函数的极大值。</p>
<h4 id="D-收获-1"><a href="#D-收获-1" class="headerlink" title="D 收获"></a>D 收获</h4><p>在解算过程中，有一个式子$P_{1} = P(y=1|x)=\sigma(w^{T}x)=\frac{1}{1+\exp(-w^{T}x)},y=1$，为什么能这么写呢？因为<strong>一个关于x的函数，得出的结果就认为是y=1的概率</strong></p>
<h3 id="4-2-5-高斯判别分析GDA"><a href="#4-2-5-高斯判别分析GDA" class="headerlink" title="4.2.5 高斯判别分析GDA"></a>4.2.5 高斯判别分析GDA</h3><blockquote>
<p>Gaussian Discriminant Analysis</p>
<p>属于，线性分类，软分类，生成式方法</p>
</blockquote>
<h4 id="背景-3"><a href="#背景-3" class="headerlink" title="背景"></a>背景</h4><p>概率的生成模型，是对联合概率分布进行建模，然后利用MAP来获得参数的最佳值。不像判别模型直接求解$P(Y|X)$的值，生成模型更关心哪个值大，不关心具体值。</p>
<h4 id="A-高斯判别分析—模型定义"><a href="#A-高斯判别分析—模型定义" class="headerlink" title="A 高斯判别分析—模型定义"></a>A 高斯判别分析—模型定义</h4><h5 id="已知-2"><a href="#已知-2" class="headerlink" title="已知"></a>已知</h5><script type="math/tex; mode=display">
\begin{align} \begin{array}{l}
Data:\{ ({x_i},{y_i})\} _{i = 1}^N\\
{x_i} \in {R^p},{y_i} \in \{ 0,1\} 
\end{array} \end{align}</script><script type="math/tex; mode=display">
\begin{align} P(Y|X) = \frac{ {P(X|Y)P(Y)}}{ {P(X)}} \propto P(X|Y)P(Y) \end{align}</script><h5 id="求-2"><a href="#求-2" class="headerlink" title="求"></a>求</h5><script type="math/tex; mode=display">
\begin{align} \hat y = \mathop {\arg \max }\limits_{y \in \{ 0,1\} } P(y|x) = \mathop {\arg \max }\limits_y P(y)P(x|y) \end{align}</script><h5 id="解-2"><a href="#解-2" class="headerlink" title="解"></a>解</h5><ol>
<li>$y\sim Bernoulli(\phi)$</li>
<li>$x|y=1\sim\mathcal{N}(\mu_1,\Sigma)$</li>
<li>$x|y=0\sim\mathcal{N}(\mu_0,\Sigma)$</li>
</ol>
<p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200522083824880.png" alt="image-20200522083824880"></p>
<p>那么独立全同的数据集最大后验概率MAP可以表示为：</p>
<script type="math/tex; mode=display">
\begin{align}
\mathop{argmax}_{\phi,\mu_0,\mu_1,\Sigma}\log p(X|Y)p(Y)=\mathop{argmax}_{\phi,\mu_0,\mu_1,\Sigma}\sum\limits_{i=1}^N (\log p(x_i|y_i)+\log p(y_i))\nonumber\\
=\mathop{argmax}_{\phi,\mu_0,\mu_1,\Sigma}\sum\limits_{i=1}^N((1-y_i)\log\mathcal{N}(\mu_0,\Sigma)+y_i\log \mathcal{N}(\mu_1,\Sigma)+y_i\log\phi+(1-y_i)\log(1-\phi))
\end{align}</script><h4 id="B-1-高斯判别分析—模型求解（求期望）"><a href="#B-1-高斯判别分析—模型求解（求期望）" class="headerlink" title="B.1 高斯判别分析—模型求解（求期望）"></a>B.1 高斯判别分析—模型求解（求期望）</h4><h5 id="已知-3"><a href="#已知-3" class="headerlink" title="已知"></a>已知</h5><blockquote>
<p>草稿上x|y=0 ~ N(u2, Σ)，实际上是x|y=0 ~ N(u0, Σ)</p>
</blockquote>
<p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200522084923196.png" alt="image-20200522084923196"></p>
<h5 id="求-3"><a href="#求-3" class="headerlink" title="求"></a>求</h5><ol>
<li>$\phi$</li>
<li>$\mu_1$</li>
<li>$\mu_0$</li>
</ol>
<h5 id="解-3"><a href="#解-3" class="headerlink" title="解"></a>解</h5><ul>
<li><p>首先对 $\phi$ 进行求解，将式子对 $\phi$ 求偏导：</p>
<script type="math/tex; mode=display">
\begin{align}\sum\limits_{i=1}^N\frac{y_i}{\phi}+\frac{y_i-1}{1-\phi}=0\nonumber\\\Longrightarrow\phi=\frac{\sum\limits_{i=1}^Ny_i}{N}=\frac{N_1}{N}\end{align}</script></li>
<li><p>然后求解 $\mu_1$：</p>
<script type="math/tex; mode=display">
\begin{align}\hat{\mu_1}&=\mathop{argmax}_{\mu_1}\sum\limits_{i=1}^Ny_i\log\mathcal{N}(\mu_1,\Sigma)\nonumber\\
&=\mathop{argmin}_{\mu_1}\sum\limits_{i=1}^Ny_i(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)
\end{align}</script><p>由于：</p>
<script type="math/tex; mode=display">
\begin{align} 
\sum\limits_{i=1}^Ny_i(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)=\sum\limits_{i=1}^Ny_ix_i^T\Sigma^{-1}x_i-2y_i\mu_1^T\Sigma^{-1}x_i+y_i\mu_1^T\Sigma^{-1}\mu_1 
\end{align}</script></li>
</ul>
<p>  求微分左边乘以 $\Sigma$ 可以得到：</p>
<script type="math/tex; mode=display">
\begin{align}\sum\limits_{i=1}^N-2y_i\Sigma^{-1}x_i+2y_i\Sigma^{-1}\mu_1=0\nonumber\\
  \Longrightarrow\mu_1=\frac{\sum\limits_{i=1}^Ny_ix_i}{\sum\limits_{i=1}^Ny_i}=\frac{\sum\limits_{i=1}^Ny_ix_i}{N_1}
  \end{align}</script><ul>
<li>求解 $\mu_0$，由于正反例是对称的，所以：<script type="math/tex; mode=display">
\begin{align} \mu_0=\frac{\sum\limits_{i=1}^N(1-y_i)x_i}{N_0} \end{align}</script></li>
</ul>
<h6 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h6><p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/微信图片_20200522085259.jpg1.jpg" alt="微信图片_20200522085259.jpg1"></p>
<h4 id="B-2-高斯判别分析—模型求解（求协方差）"><a href="#B-2-高斯判别分析—模型求解（求协方差）" class="headerlink" title="B.2 高斯判别分析—模型求解（求协方差）"></a>B.2 高斯判别分析—模型求解（求协方差）</h4><h5 id="已知-4"><a href="#已知-4" class="headerlink" title="已知"></a>已知</h5><p>最为困难的是求解 $\Sigma$</p>
<h5 id="求-4"><a href="#求-4" class="headerlink" title="求"></a>求</h5><ol>
<li>$\Sigma$</li>
</ol>
<h5 id="解-4"><a href="#解-4" class="headerlink" title="解"></a>解</h5><p>最为困难的是求解 $\Sigma$，我们的模型假设对正反例采用相同的协方差矩阵，当然从上面的求解中我们可以看到，即使采用不同的矩阵也不会影响之前的三个参数。首先我们有：</p>
<script type="math/tex; mode=display">
\begin{align}
\sum\limits_{i=1}^N\log\mathcal{N}(\mu,\Sigma)&=\sum\limits_{i=1}^N\log(\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}})+(-\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu))\nonumber\\
&=Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}Trace((x_i-\mu)^T\Sigma^{-1}(x_i-\mu))\nonumber\\
&=Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}Trace((x_i-\mu)(x_i-\mu)^T\Sigma^{-1})\nonumber\\
&=Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}NTrace(S\Sigma^{-1})
\end{align}</script><p>在这个表达式中，我们在标量上加入迹从而可以交换矩阵的顺序，对于包含绝对值和迹的表达式的导数，我们有：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial}{\partial A}(|A|)&=|A|A^{-1}
\end{align}</script><script type="math/tex; mode=display">
\begin{align} \frac{\partial}{\partial A}Trace(AB)&=B^T \end{align}</script><p>因此：</p>
<script type="math/tex; mode=display">
\begin{align}[\sum\limits_{i=1}^N((1-y_i)\log\mathcal{N}(\mu_0,\Sigma)+y_i\log \mathcal{N}(\mu_1,\Sigma)]'
\nonumber\\=Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}N_1Trace(S_1\Sigma^{-1})-\frac{1}{2}N_2Trace(S_2\Sigma^{-1})
\end{align}</script><p>其中，$S_1,S_2$ 分别为两个类数据内部的协方差矩阵，于是：</p>
<script type="math/tex; mode=display">
\begin{align}N\Sigma^{-1}-N_1S_1^T\Sigma^{-2}-N_2S_2^T\Sigma^{-2}=0\nonumber
\\\Longrightarrow\Sigma=\frac{N_1S_1+N_2S_2}{N}
\end{align}</script><p>这里应用了类协方差矩阵的对称性。</p>
<h6 id="推导过程-1"><a href="#推导过程-1" class="headerlink" title="推导过程"></a>推导过程</h6><p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200522085726438.png" alt="image-20200522085726438"></p>
<h4 id="C-收获-1"><a href="#C-收获-1" class="headerlink" title="C 收获"></a>C 收获</h4><p>于是我们就利用最大后验的方法求得了我们模型假设里面的所有参数，根据模型，可以得到联合分布，也就可以得到用于推断的条件分布了。</p>
<h3 id="4-2-6-朴素贝叶斯分类器"><a href="#4-2-6-朴素贝叶斯分类器" class="headerlink" title="4.2.6 朴素贝叶斯分类器"></a>4.2.6 朴素贝叶斯分类器</h3><blockquote>
<p>naive Bayes Classifer</p>
<p>属于，线性分类，软分类，判别式方法</p>
</blockquote>
<h4 id="背景-4"><a href="#背景-4" class="headerlink" title="背景"></a>背景</h4><p><strong>思想：</strong><code>朴素贝叶斯假设</code>-&gt;<code>条件独立性假设</code>-&gt;<code>最简单的概率图(有向图)模型</code></p>
<p><strong>做假设的动机：</strong>简化运算</p>
<script type="math/tex; mode=display">
\begin{align} P(Y|X) = \frac{ {P(X|Y)P(Y)}}{ {P(X)}} \propto P(X|Y)P(Y) \end{align}</script><p>主要是简化$P(X|Y)$</p>
<h4 id="A-已知-2"><a href="#A-已知-2" class="headerlink" title="A 已知"></a>A 已知</h4><p>上面的高斯判别分析的是对数据集的分布作出了高斯分布的假设，同时引入伯努利分布作为类先验，从而利用最大后验求得这些假设中的参数。</p>
<p>朴素贝叶斯对数据的属性之间的关系作出了假设，一般地，我们有需要得到 $p(x|y)$ 这个概率值，由于 $x$ 有 $p$ 个维度，因此需要对这么多的维度的联合概率进行采样，但是我们知道这么高维度的空间中采样需要的样本数量非常大才能获得较为准确的概率近似。</p>
<h4 id="B-求-2"><a href="#B-求-2" class="headerlink" title="B 求"></a>B 求</h4><p>简化$P(X|Y)$</p>
<h4 id="C-解-2"><a href="#C-解-2" class="headerlink" title="C 解"></a>C 解</h4><p>在一般的有向概率图模型中，对各个属性维度之间的条件独立关系作出了不同的假设，其中最为简单的一个假设就是在朴素贝叶斯模型描述中的条件独立性假设。</p>
<script type="math/tex; mode=display">
\begin{align} p(x|y)=\prod\limits_{i=1}^pp(x_i|y) \end{align}</script><p>即：</p>
<script type="math/tex; mode=display">
\begin{align} x_i\perp x_j|y,\forall\  i\ne j \end{align}</script><p>于是利用贝叶斯定理，对于单次观测：</p>
<script type="math/tex; mode=display">
\begin{align}
p(y|x)=\frac{p(x|y)p(y)}{p(x)}=\frac{\prod\limits_{i=1}^pp(x_i|y)p(y)}{p(x)}
\end{align}</script><p>对于单个维度的条件概率以及类先验作出进一步的假设：</p>
<p>1.$x_i$为连续变量：$p(x_i|y)=\mathcal{N}(\mu_i,\sigma_i^2)$  </p>
<p>2.<script type="math/tex">x_i</script>为离散变量：类别分布（Categorical）：  <script type="math/tex">p(x_i=i|y)=\theta_i,\sum\limits_{i=1}^K\theta_i=1</script>   </p>
<p>3.$p(y)=\phi^y(1-\phi)^{1-y}$  </p>
<p>对这些参数的估计，常用 MLE 的方法直接在数据集上估计，由于不需要知道各个维度之间的关系，因此，所需数据量大大减少了。估算完这些参数，再代入贝叶斯定理中得到类别的后验分布。</p>
<h4 id="D-收获-2"><a href="#D-收获-2" class="headerlink" title="D 收获"></a>D 收获</h4><p><img src="/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/image-20200522092643160.png" alt="image-20200522092643160"></p>
<h2 id="4-3-问题"><a href="#4-3-问题" class="headerlink" title="4.3 问题"></a>4.3 问题</h2><h3 id="4-3-1-线性分类之软输出的判别式和生成式区别是什么？"><a href="#4-3-1-线性分类之软输出的判别式和生成式区别是什么？" class="headerlink" title="4.3.1 线性分类之软输出的判别式和生成式区别是什么？"></a>4.3.1 线性分类之软输出的判别式和生成式区别是什么？</h3><p>判别式，是直接对$P(Y|X)$进行求解，通过直接对$P(Y|X)$建模，求解出$P(Y=1|X)和P(Y=0|X)$的值。即，判别式更关心值的大小</p>
<p>生成式，不直接对$P(Y|X)$进行求解，而是利用贝叶斯公式，$P(Y|X){\rm{ = }}\frac{ {P(X|Y)P(Y)}}{ {P(X)}}$，通过计算$P(Y|X)P(Y)$来得到$P(Y|X)$。生成式更关心$P(Y=1|X)和P(Y=0|X)$谁大，不关心具体值。</p>
<h2 id="4-4-小结"><a href="#4-4-小结" class="headerlink" title="4.4 小结"></a>4.4 小结</h2><p>分类的本质核心任务：对于给定的x，输出y属于0类，还是属于1类。用数学描述就是求$P(y|x)$。</p>
<blockquote>
<p>以下来自<code>tsyw</code>的github库笔记</p>
</blockquote>
<p>分类任务分为两类，对于需要直接输出类别的任务，感知机算法中我们在线性模型的基础上加入符号函数作为激活函数，那么就能得到这个类别，但是符号函数不光滑，于是我们采用错误驱动的方式，引入  <script type="math/tex">\sum\limits_{x_i\in\mathcal{D}_{wrong}}-y_iw^Tx_i</script>  作为损失函数，然后最小化这个误差，采用批量随机梯度下降的方法来获取最佳的参数值。而在线性判别分析中，我们将线性模型看作是数据点在某一个方向的投影，采用类内小，类间大的思路来定义损失函数，其中类内小定义为两类数据的方差之和，类间大定义为两类数据中心点的间距，对损失函数求导得到参数的方向，这个方向就是  $S<em>w^{-1}(\overline x</em>{c1}-\overline x_{c2})$ ，其中  $S_w$  为原数据集两类的方差之和。</p>
<p>另一种任务是输出分类的概率，对于概率模型，我们有两种方案，第一种是判别模型，也就是直接对类别的条件概率建模，将线性模型套入 Logistic 函数中，我们就得到了 Logistic 回归模型，这里的概率解释是两类的联合概率比值的对数是线性的，我们定义的损失函数是交叉熵（等价于 MLE），对这个函数求导得到   <script type="math/tex">\frac{1}{N}\sum\limits_{i=1}^N(y_i-p_1)x_i</script>   ，同样利用批量随机梯度（上升）的方法进行优化。第二种是生成模型，生成模型引入了类别的先验，在高斯判别分析中，我们对数据集的数据分布作出了假设，其中类先验是二项分布，而每一类的似然是高斯分布，对这个联合分布的对数似然进行最大化就得到了参数，  <script type="math/tex">\frac{\sum\limits_{i=1}^Ny_ix_i}{N_1},\frac{\sum\limits_{i=1}^N(1-y_i)x_i}{N_0},\frac{N_1S_1+N_2S_2}{N},\frac{N_1}{N}</script>  。在朴素贝叶斯中，我们进一步对属性的各个维度之间的依赖关系作出假设，条件独立性假设大大减少了数据量的需求。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] shuhuai008. 【机器学习】【白板推导系列】【合集 1～23】. bilibili. 2019.<br><a href="https://www.bilibili.com/video/BV1aE411o7qd?p=13" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1aE411o7qd?p=13</a></p>
<p>[2] tsyw. <a href="https://github.com/tsyw/MachineLearningNotes" target="_blank" rel="noopener">https://github.com/tsyw/MachineLearningNotes</a></p>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    Donate
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="Ayzp Alipay">
        <p>Alipay</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Ayzp
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="https://ayzp.github.io/2020/05/27/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/" title="数学-机器学习-线性分类">https://ayzp.github.io/2020/05/27/数学-机器学习-线性分类/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%95%B0%E5%AD%A6/" rel="tag"> <i class="fa fa-tag"></i> 数学</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"> <i class="fa fa-tag"></i> 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/05/15/%E6%95%B0%E5%AD%A6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="prev" title="数学-机器学习-线性回归">
      <i class="fa fa-chevron-left"></i> 数学-机器学习-线性回归
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#四-线性分类"><span class="nav-number">1.</span> <span class="nav-text">四 线性分类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-简介"><span class="nav-number">1.1.</span> <span class="nav-text">4.1 简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-1-思维导图"><span class="nav-number">1.1.1.</span> <span class="nav-text">4.1.1 思维导图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-2-线性分类"><span class="nav-number">1.1.2.</span> <span class="nav-text">4.2.2 线性分类</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-内容"><span class="nav-number">1.2.</span> <span class="nav-text">4.2 内容</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-1-背景"><span class="nav-number">1.2.1.</span> <span class="nav-text">4.2.1 背景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-2-感知机"><span class="nav-number">1.2.2.</span> <span class="nav-text">4.2.2 感知机</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#背景"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-已知"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">A 已知</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-求"><span class="nav-number">1.2.2.3.</span> <span class="nav-text">B 求</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#C-解"><span class="nav-number">1.2.2.4.</span> <span class="nav-text">C 解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-收获"><span class="nav-number">1.2.2.5.</span> <span class="nav-text">D 收获</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-3-线性判别分析LDA"><span class="nav-number">1.2.3.</span> <span class="nav-text">4.2.3 线性判别分析LDA</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#背景-1"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">背景</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#思想："><span class="nav-number">1.2.3.1.1.</span> <span class="nav-text">思想：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#投影"><span class="nav-number">1.2.3.1.2.</span> <span class="nav-text">投影</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-线性判别分析——模型定义"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">A 线性判别分析——模型定义</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#已知"><span class="nav-number">1.2.3.2.1.</span> <span class="nav-text">已知</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#求"><span class="nav-number">1.2.3.2.2.</span> <span class="nav-text">求</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#解"><span class="nav-number">1.2.3.2.3.</span> <span class="nav-text">解</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-线性判别分析——模型求解"><span class="nav-number">1.2.3.3.</span> <span class="nav-text">B 线性判别分析——模型求解</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#已知-1"><span class="nav-number">1.2.3.3.1.</span> <span class="nav-text">已知</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#求-1"><span class="nav-number">1.2.3.3.2.</span> <span class="nav-text">求</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#解-1"><span class="nav-number">1.2.3.3.3.</span> <span class="nav-text">解</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#C-收获"><span class="nav-number">1.2.3.4.</span> <span class="nav-text">C 收获</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-4-逻辑回归"><span class="nav-number">1.2.4.</span> <span class="nav-text">4.2.4 逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#背景-2"><span class="nav-number">1.2.4.1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-已知-1"><span class="nav-number">1.2.4.2.</span> <span class="nav-text">A 已知</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-求-1"><span class="nav-number">1.2.4.3.</span> <span class="nav-text">B 求</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#C-解-1"><span class="nav-number">1.2.4.4.</span> <span class="nav-text">C 解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-收获-1"><span class="nav-number">1.2.4.5.</span> <span class="nav-text">D 收获</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-5-高斯判别分析GDA"><span class="nav-number">1.2.5.</span> <span class="nav-text">4.2.5 高斯判别分析GDA</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#背景-3"><span class="nav-number">1.2.5.1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-高斯判别分析—模型定义"><span class="nav-number">1.2.5.2.</span> <span class="nav-text">A 高斯判别分析—模型定义</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#已知-2"><span class="nav-number">1.2.5.2.1.</span> <span class="nav-text">已知</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#求-2"><span class="nav-number">1.2.5.2.2.</span> <span class="nav-text">求</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#解-2"><span class="nav-number">1.2.5.2.3.</span> <span class="nav-text">解</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-1-高斯判别分析—模型求解（求期望）"><span class="nav-number">1.2.5.3.</span> <span class="nav-text">B.1 高斯判别分析—模型求解（求期望）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#已知-3"><span class="nav-number">1.2.5.3.1.</span> <span class="nav-text">已知</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#求-3"><span class="nav-number">1.2.5.3.2.</span> <span class="nav-text">求</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#解-3"><span class="nav-number">1.2.5.3.3.</span> <span class="nav-text">解</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#推导过程"><span class="nav-number">1.2.5.3.3.1.</span> <span class="nav-text">推导过程</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-2-高斯判别分析—模型求解（求协方差）"><span class="nav-number">1.2.5.4.</span> <span class="nav-text">B.2 高斯判别分析—模型求解（求协方差）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#已知-4"><span class="nav-number">1.2.5.4.1.</span> <span class="nav-text">已知</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#求-4"><span class="nav-number">1.2.5.4.2.</span> <span class="nav-text">求</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#解-4"><span class="nav-number">1.2.5.4.3.</span> <span class="nav-text">解</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#推导过程-1"><span class="nav-number">1.2.5.4.3.1.</span> <span class="nav-text">推导过程</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#C-收获-1"><span class="nav-number">1.2.5.5.</span> <span class="nav-text">C 收获</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-6-朴素贝叶斯分类器"><span class="nav-number">1.2.6.</span> <span class="nav-text">4.2.6 朴素贝叶斯分类器</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#背景-4"><span class="nav-number">1.2.6.1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-已知-2"><span class="nav-number">1.2.6.2.</span> <span class="nav-text">A 已知</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-求-2"><span class="nav-number">1.2.6.3.</span> <span class="nav-text">B 求</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#C-解-2"><span class="nav-number">1.2.6.4.</span> <span class="nav-text">C 解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-收获-2"><span class="nav-number">1.2.6.5.</span> <span class="nav-text">D 收获</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-问题"><span class="nav-number">1.3.</span> <span class="nav-text">4.3 问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-1-线性分类之软输出的判别式和生成式区别是什么？"><span class="nav-number">1.3.1.</span> <span class="nav-text">4.3.1 线性分类之软输出的判别式和生成式区别是什么？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-小结"><span class="nav-number">1.4.</span> <span class="nav-text">4.4 小结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">1.5.</span> <span class="nav-text">参考文献</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ayzp"
      src="/images/headphoto.png">
  <p class="site-author-name" itemprop="name">Ayzp</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">20</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ayzp" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ayzp" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1922428186@qq.com" title="E-Mail → mailto:1922428186@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.jianshu.com/u/808fb7830936" title="简书 → https:&#x2F;&#x2F;www.jianshu.com&#x2F;u&#x2F;808fb7830936" rel="noopener" target="_blank"><i class="fa fa-heartbeat fa-fw"></i>简书</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/ALexander_Monster" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;ALexander_Monster" rel="noopener" target="_blank"><i class="fa fa-book fa-fw"></i>CSDN</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">YZP</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">39k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">35 mins.</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
